{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: 데이터셋 준비 및 Data Loader 설정"
      ],
      "metadata": {
        "id": "LAn_mwQzzmIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvlJ30bb0onX",
        "outputId": "de17c3e5-c716-4ac5-bdfa-8719794e5841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 및 토크나이저 설정\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 데이터셋 로드\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "# BERT Tokenizer 로드\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# collate_fn 함수 수정\n",
        "def collate_fn(batch):\n",
        "    max_len = 400\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for row in batch:\n",
        "        tokens = tokenizer(row['text'], padding=True, truncation=True, max_length=max_len, return_tensors='pt')['input_ids'].squeeze(0)\n",
        "        # 마지막 두 토큰 제외한 입력으로 설정\n",
        "        inputs = tokens[:-2]\n",
        "        # 마지막에서 두 번째 토큰을 레이블로 설정\n",
        "        label = tokens[-2]\n",
        "        texts.append(inputs)\n",
        "        labels.append(label)\n",
        "\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "train_loader = DataLoader(ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "aHGWaFFvzvNg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Transformer 구성 요소 구현"
      ],
      "metadata": {
        "id": "nuI9HIH0zyJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Multi-head Attention 클래스 정의\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # D = H * D'\n",
        "\n",
        "        # Q, K, V의 Linear layer\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 최종적으로 결합한 결과를 위한 output layer\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Softmax 함수\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 1. Q, K, V 계산 (batch_size, seq_len, d_model)\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "\n",
        "        # 2. H개의 head로 나누기 (batch_size, seq_len, d_model) -> (batch_size, H, seq_len, D')\n",
        "        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 3. Scaled dot-product attention 계산\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 4. Mask 적용\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 5. Attention 계산 후 V와 곱하기\n",
        "        attention = self.softmax(scores)\n",
        "        context = torch.matmul(attention, v)\n",
        "\n",
        "        # 6. H개의 head 결합하기\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 7. Output layer 통과\n",
        "        output = self.wo(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Positional encoding 정의\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "# Transformer Layer 구현\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dff, dropout_rate=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # MHA -> Dropout -> Residual -> Layer Norm\n",
        "        attn_output = self.mha(x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(attn_output + x)\n",
        "\n",
        "        # Feed Forward -> Dropout -> Residual -> Layer Norm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(ffn_output + out1)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "ALTkzFwc2HDd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: TextClassifier 및 학습 설정"
      ],
      "metadata": {
        "id": "gSMvDT_e2JG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TextClassifier 정의: 마지막 토큰을 예측하는 모델\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, dff, max_len, dropout_rate=0.1):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "        self.layers = nn.ModuleList([TransformerLayer(d_model, n_heads, dff, dropout_rate) for _ in range(n_layers)])\n",
        "        self.classifier = nn.Linear(d_model, vocab_size)  # 마지막 토큰의 ID를 예측하기 위해 vocab_size로 조정\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        seq_len = x.shape[1]\n",
        "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = x[:, -1]  # 마지막 토큰에 대한 예측\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Optimizer 및 손실 함수 설정\n",
        "import torch.optim as optim\n",
        "\n",
        "model = TextClassifier(len(tokenizer.vocab), 128, 5, 4, 512, max_len=400, dropout_rate=0.1).to('cuda')\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 학습 루프\n",
        "def train_model(model, train_loader, test_loader, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(inputs)\n",
        "            loss = loss_fn(preds, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        train_acc = calculate_accuracy(model, train_loader)\n",
        "        test_acc = calculate_accuracy(model, test_loader)\n",
        "        print(f\"Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# 정확도 계산 함수\n",
        "def calculate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            preds = model(inputs)\n",
        "            predicted = torch.argmax(preds, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# 모델 학습 실행\n",
        "train_model(model, train_loader, test_loader, n_epochs=50)"
      ],
      "metadata": {
        "id": "TKJzE4uH2MFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1631cf6-0d74-44df-d5c1-de38ab9aea99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 1228.4641\n",
            "Train Accuracy: 0.5742 | Test Accuracy: 0.5767\n",
            "Epoch 2 | Loss: 1002.9105\n",
            "Train Accuracy: 0.5775 | Test Accuracy: 0.5796\n",
            "Epoch 3 | Loss: 963.6414\n",
            "Train Accuracy: 0.5814 | Test Accuracy: 0.5813\n",
            "Epoch 4 | Loss: 931.7001\n",
            "Train Accuracy: 0.5886 | Test Accuracy: 0.5833\n",
            "Epoch 5 | Loss: 896.8537\n",
            "Train Accuracy: 0.5963 | Test Accuracy: 0.5836\n",
            "Epoch 6 | Loss: 837.9196\n",
            "Train Accuracy: 0.6169 | Test Accuracy: 0.5833\n",
            "Epoch 7 | Loss: 779.2275\n",
            "Train Accuracy: 0.6356 | Test Accuracy: 0.5821\n",
            "Epoch 8 | Loss: 726.8249\n",
            "Train Accuracy: 0.6557 | Test Accuracy: 0.5840\n",
            "Epoch 9 | Loss: 688.7736\n",
            "Train Accuracy: 0.6629 | Test Accuracy: 0.5863\n",
            "Epoch 10 | Loss: 657.6734\n",
            "Train Accuracy: 0.6648 | Test Accuracy: 0.5825\n",
            "Epoch 11 | Loss: 631.7366\n",
            "Train Accuracy: 0.6744 | Test Accuracy: 0.5835\n",
            "Epoch 12 | Loss: 609.4695\n",
            "Train Accuracy: 0.6794 | Test Accuracy: 0.5828\n",
            "Epoch 13 | Loss: 591.0056\n",
            "Train Accuracy: 0.6931 | Test Accuracy: 0.5856\n",
            "Epoch 14 | Loss: 568.5691\n",
            "Train Accuracy: 0.7019 | Test Accuracy: 0.5834\n",
            "Epoch 15 | Loss: 547.3300\n",
            "Train Accuracy: 0.7133 | Test Accuracy: 0.5876\n",
            "Epoch 16 | Loss: 523.2385\n",
            "Train Accuracy: 0.7255 | Test Accuracy: 0.5848\n",
            "Epoch 17 | Loss: 497.3960\n",
            "Train Accuracy: 0.7392 | Test Accuracy: 0.5902\n",
            "Epoch 18 | Loss: 471.3938\n",
            "Train Accuracy: 0.7520 | Test Accuracy: 0.5884\n",
            "Epoch 19 | Loss: 442.5467\n",
            "Train Accuracy: 0.7583 | Test Accuracy: 0.5872\n",
            "Epoch 20 | Loss: 425.2281\n",
            "Train Accuracy: 0.7676 | Test Accuracy: 0.5884\n",
            "Epoch 21 | Loss: 400.6441\n",
            "Train Accuracy: 0.7672 | Test Accuracy: 0.5826\n",
            "Epoch 22 | Loss: 386.9045\n",
            "Train Accuracy: 0.7829 | Test Accuracy: 0.5877\n",
            "Epoch 23 | Loss: 366.9930\n",
            "Train Accuracy: 0.7924 | Test Accuracy: 0.5906\n",
            "Epoch 24 | Loss: 352.6731\n",
            "Train Accuracy: 0.7922 | Test Accuracy: 0.5867\n",
            "Epoch 25 | Loss: 336.0622\n",
            "Train Accuracy: 0.8028 | Test Accuracy: 0.5834\n",
            "Epoch 26 | Loss: 319.3021\n",
            "Train Accuracy: 0.8167 | Test Accuracy: 0.5830\n",
            "Epoch 27 | Loss: 301.8750\n",
            "Train Accuracy: 0.8191 | Test Accuracy: 0.5822\n",
            "Epoch 28 | Loss: 287.6096\n",
            "Train Accuracy: 0.8290 | Test Accuracy: 0.5790\n",
            "Epoch 29 | Loss: 277.1304\n",
            "Train Accuracy: 0.8372 | Test Accuracy: 0.5752\n",
            "Epoch 30 | Loss: 256.9771\n",
            "Train Accuracy: 0.8471 | Test Accuracy: 0.5643\n",
            "Epoch 31 | Loss: 246.4808\n",
            "Train Accuracy: 0.8535 | Test Accuracy: 0.5749\n",
            "Epoch 32 | Loss: 232.7775\n",
            "Train Accuracy: 0.8709 | Test Accuracy: 0.5734\n",
            "Epoch 33 | Loss: 214.9049\n",
            "Train Accuracy: 0.8769 | Test Accuracy: 0.5694\n",
            "Epoch 34 | Loss: 203.4362\n",
            "Train Accuracy: 0.8686 | Test Accuracy: 0.5690\n",
            "Epoch 35 | Loss: 196.7454\n",
            "Train Accuracy: 0.8791 | Test Accuracy: 0.5303\n",
            "Epoch 36 | Loss: 187.7982\n",
            "Train Accuracy: 0.9049 | Test Accuracy: 0.5415\n",
            "Epoch 37 | Loss: 176.5423\n",
            "Train Accuracy: 0.9094 | Test Accuracy: 0.5445\n",
            "Epoch 38 | Loss: 164.6667\n",
            "Train Accuracy: 0.9116 | Test Accuracy: 0.5381\n",
            "Epoch 39 | Loss: 154.5664\n",
            "Train Accuracy: 0.9140 | Test Accuracy: 0.5569\n",
            "Epoch 40 | Loss: 144.5114\n",
            "Train Accuracy: 0.9152 | Test Accuracy: 0.5169\n",
            "Epoch 41 | Loss: 143.5280\n",
            "Train Accuracy: 0.9383 | Test Accuracy: 0.5548\n",
            "Epoch 42 | Loss: 135.7310\n",
            "Train Accuracy: 0.9289 | Test Accuracy: 0.5659\n",
            "Epoch 43 | Loss: 125.3391\n",
            "Train Accuracy: 0.9463 | Test Accuracy: 0.5502\n",
            "Epoch 44 | Loss: 117.5674\n",
            "Train Accuracy: 0.9530 | Test Accuracy: 0.5475\n",
            "Epoch 45 | Loss: 111.3365\n",
            "Train Accuracy: 0.9512 | Test Accuracy: 0.5526\n",
            "Epoch 46 | Loss: 108.7648\n",
            "Train Accuracy: 0.9478 | Test Accuracy: 0.5437\n",
            "Epoch 47 | Loss: 105.3455\n",
            "Train Accuracy: 0.9516 | Test Accuracy: 0.5208\n",
            "Epoch 48 | Loss: 98.8507\n",
            "Train Accuracy: 0.9617 | Test Accuracy: 0.5426\n",
            "Epoch 49 | Loss: 93.3677\n",
            "Train Accuracy: 0.9569 | Test Accuracy: 0.5178\n",
            "Epoch 50 | Loss: 91.2367\n",
            "Train Accuracy: 0.9623 | Test Accuracy: 0.5481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_predictions(model, test_loader, tokenizer, num_samples=10):\n",
        "    model.eval()\n",
        "    samples = random.sample(list(test_loader), num_samples)\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(samples):\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        preds = model(inputs)\n",
        "        predicted_tokens = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Input Tokens: {tokenizer.decode(inputs[0].cpu().numpy())}\")\n",
        "        print(f\"Predicted Token: {tokenizer.decode([predicted_tokens[0]])}\")\n",
        "        print(f\"Actual Token: {tokenizer.decode([labels[0].cpu().numpy()])}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "# 임의의 10개 문장으로 예측 결과 확인\n",
        "test_predictions(model, test_loader, tokenizer, num_samples=10)\n"
      ],
      "metadata": {
        "id": "KPs3biVt2otj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e45d3fd-ac5d-4f82-92a4-7e740f4161fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Input Tokens: [CLS] people with an aversion to gore may find some scenes hard going, but the thing is far from being simply a horror classic. the fact that the extraordinary special effects stand up against most modern day cgi is only a small part of why this movie is, finally, rightfully regarded as a masterpiece. technically brilliant in its camera - work and editing, superbly scripted and acted, one of the best openings, one of the best endings, tension and paranoia sustained throughout ( with countless viewings ), an excellent soundtrack, and open to multiple readings and analogy, there simply aren't enough superlatives to do this film justice. absolutely essential viewing [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: .\n",
            "Actual Token: .\n",
            "-----\n",
            "Sample 2:\n",
            "Input Tokens: [CLS] sharpe's honour for the uninitiated, is the fifth entry in a series of tv movies focusing on an english army rifleman during the napoleonic wars and based on the books by bernard cornwell ( which i strongly recommend reading ). if you were to start by watching this particular one though, you'd get the impression that sharpe is not so much a soldier as the very centrifugal force which the rest of the army revolves around. should that be the case, i'd recommend starting with earlier chapters like sharpe's eagle or sharpe's company, but this is a worthy choice for a second viewing. < br / > < br / > the story this time is all about the espionage side of things. with the french army retreating in disarray from spain, major ducos, the slimy spy master spots an opportunity to turn the situation round. by pinning the murder of a spanish marques on richard sharpe, hero of the british army, the fragile british / spanish alliance will start to crumble and things will turn around again. when the spanish nobles come to wellington crying for sharpe's blood though, the english general is less than willing to hang his best soldier so fakes his death and soon, he's off on a secret mission behind enemy lines to find out who masterminded the plot. surprisingly enough for a sharpe film as well, there's a gorgeous woman to be rescued along the way, fancy that. < br / > < br / > what this results in of course is a more adventure style approach. the concentration is less on the workings of the english military with sharpe as the figurehead and concentrates more on his escapades in the countryside, dodging french patrols, hob nobbing with the spanish guerrillas and getting involved in daring escapes from fortified military positions. sergeant harper, his loyal right hand man accompanies him naturally but the rest of the riflemen remain in the camp unaware\n",
            "Predicted Token: the\n",
            "Actual Token: their\n",
            "-----\n",
            "Sample 3:\n",
            "Input Tokens: [CLS] when i saw this movie at age 6, it was in the childrens'section at erols video because it was animation. we watched it and it was a whole different ball game! a very violent story and graphic deaths are very entertaining and compelling, but not for children. avoid for family viewing, my mom nearly had a heart attack and ripped the video apart [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: !\n",
            "Actual Token: !\n",
            "-----\n",
            "Sample 4:\n",
            "Input Tokens: [CLS] i never heard of this film when it first came out. it must have sunk immediately. : o ) i saw it on cable while sick in hospital so i hardly had enough energy to watch it, let alone turn the channel. better choice than the style channel. ; 0 (. filmed on location, this travelogue should have been on the travel channel. the plot is recycled from ship board farces of the thirties and forties. the cast seems to have been recycled from the fifties. donald o'connor, star of musicals and edward mulhare as a card shark. as to the main cast, walter matthau is still playing the same part as he did in guys and dolls or was it the one about the orphan girl? wiseacre irresponsible gambler and rounder. but it just doesn't take with a man of his age. as to jack lemmon, he plays his part so straight, he can hardly dip and glide when dancing. and as mentioned, dyan cannon is outstandingly attractive as another swindler sailing with her mother who thinks walter is rich, while he thinks she is rich. elaine stritch plays dyan's mother, another retread from the fifties. the most fun is the running feud between brent spiner as the domineering and snotty cruise director who immediately spots walter as a poor dancer, and spends his time trying to get him dismissed so he will have to pay for his free passage. in the end, though he receives his comeuppances. meanwhile jack mopes about, meets an attractive woman, with mutual attraction, but their affair is broken up by walter's lies that jack is a doctor, when he was actually a retired department store buyer. but finally, the two men take to the sea in a rubber boat to intercept her seaplane and all is well. there does not seem to be any principal player under the age of fifty\n",
            "Predicted Token: with\n",
            "Actual Token: .\n",
            "-----\n",
            "Sample 5:\n",
            "Input Tokens: [CLS] while caricatures and / or references to entertainment industry people or things or even brands of products is usually a staple in shorts like this one, they aren't used in quantity here. most of the individual gags are rather generic. as i'm going to give examples, there will be spoilers below : < br / > < br / > there are only three ( well, technically four - there's a quick one at the very end of the cartoon ) caricatures that i spotted, which is kind of low for this type of short, though one is a featured character with a fair amount of activity. they are jack benny ( as jack bunny ), leopold stowkowski and the inimitable ned sparks ( as a crab on a can - chances are very good that, if a crab was involved in a warner brothers short in the 1930s - 1940s, the caricature used would be ned sparks ). there are also references to billy rose's aquacade and a riff on a radio show character called \" henry aldrich \" ( coming, mother! ), a play on superman ( superguy here ) and the villain is a take - off on \" king kong \". that's it for that kind of gag. < br / > < br / > the products themselves are mostly generic and the gags are more plays on basic items in unusual situations, such as turtles coming off of cans of soup to attack the villain as tanks, tomato soup cans doing \" the can - can ', gingerbread men who turn into paratroopers, using tissues for parachutes and so on. the gags are very good and it's an excellent example of a bob clampett cartoon. clampett had hit his stride as a director by this point and while it isn't anywhere near his best work, it's nothing to sneeze at either. this short can be found on looney tunes golden collection\n",
            "Predicted Token: the\n",
            "Actual Token: ,\n",
            "-----\n",
            "Sample 6:\n",
            "Input Tokens: [CLS] not only did the effects and acting in this movie bite, but the story was terrible. < br / > < br / > a scientist discovers that a comet fragment will hit the moon... world leaders ignore him... he builds a shelter... then, everyone is upset that he is \" playing god \". < br / > < br / > how lame! he built the thing, why is everyone \" entitled \" to access? totally lame story, don't waste your time [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: !\n",
            "Actual Token: !\n",
            "-----\n",
            "Sample 7:\n",
            "Input Tokens: [CLS] quite possibly one of the greatest wastes of celluloid of the past 100 years. not only does it suffer from a painfully ( and enormously predictable ) disjointed script, but it's clearly a carbon - copy of alien ii. within five minutes i had correctly predicted who would die and who wouldn't ( and in which order ). the special effects are laughable ; there is a scene where one crew member is mauled ( unconvincingly ) by two krites that look like a pair of teddy - bears, and the sparse humor is misplaced and dire. there are better things to do with a vcr remote than use it to watch this movie [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: .\n",
            "Actual Token: .\n",
            "-----\n",
            "Sample 8:\n",
            "Input Tokens: [CLS] little quentin seems to have mastered the art of having the cake and eating it. < br / > < br / > as usual, the pure sadistic display can be explained as a clever thought - provoking way of sending violence back into the audience's face. < br / > < br / > sure, mr tarantino. violence is baaad. sadism is baaad. it is well worth wading in it to make that point. how very brilliant. < br / > < br / > the juvenile part of the audience may well not be clever enough to follow all the smart references to higher levels of consciousness though, but i'm confident they'll see the light one day. < br / > < br / > thanks for making this little world of ours a little better. you deserve a medal [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: .\n",
            "Actual Token: .\n",
            "-----\n",
            "Sample 9:\n",
            "Input Tokens: [CLS] i consented to watching this movie with a group of friends despite my extreme dislike for horror movies. however, it was not the shock of a monster that turned me off this movie, it was the horrendous acting and absolutely disgusting ending. within, or the cavern, has no redeeming qualities - it is poorly made, laughably scripted, sickeningly bloody and the inclusion of the gratuitous final scene repulses me. no, it is not my dislike for horror movies that makes me hate this film - i've seen such wonderful teen horrors as \" house of wax \", its the fact that the film leaves you with the awful understanding that by renting the video, you are supporting the creators of [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: !\n",
            "Actual Token: within\n",
            "-----\n",
            "Sample 10:\n",
            "Input Tokens: [CLS] ok, why complain about this movie? it's fiction. deal with it. if you want to see the biography, go watch it. this is an original, fictionalized version of what happened in wisconsin. people who are obsessed will complain about this, as they do every other deviation of the facts. sad but true. i think making kane hodder the man in which the film is named after was a great idea. i thought it wasn't so good at first, i'll be honest. but that just made it even scarier. if you like kane hodder, ed gein or movies based on real events, i think this is a good movie. but if you're obsessed ( like some other people ) stay away from this movie and all others [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Token: .\n",
            "Actual Token: .\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train에서는 지속적으로 정확도가 좋아지는 모습을 보이지만, Test에서는 과적합(Overfitting) 문제 때문인지 일정 구간부터 정확도가 더 이상 오르지 않고 오히려 내려가는 모습까지 보입니다."
      ],
      "metadata": {
        "id": "iEmZyaEY938F"
      }
    }
  ]
}